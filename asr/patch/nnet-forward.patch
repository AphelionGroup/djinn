--- bkp/nnet-forward.cc.bkp	2014-11-08 22:02:48.572036592 -0500
+++ src/nnet-forward.cc	2015-01-17 13:49:49.522571524 -0500
@@ -18,23 +18,29 @@
 // limitations under the License.
 
 #include <limits>
+#include <ctime>
 
 #include "nnet/nnet-nnet.h"
-#include "nnet/nnet-loss.h"
+//#include "nnet/nnet-loss.h"
 #include "nnet/nnet-pdf-prior.h"
+#include "nnet/nnet-rbm.h"
 #include "base/kaldi-common.h"
 #include "util/common-utils.h"
 #include "base/timer.h"
+#include "socket.h"
 
+#define DEBUG 0
 
 int main(int argc, char *argv[]) {
   using namespace kaldi;
   using namespace kaldi::nnet1;
   try {
+
     const char *usage =
         "Perform forward pass through Neural Network.\n"
         "\n"
         "Usage:  nnet-forward [options] <model-in> <feature-rspecifier> <feature-wspecifier>\n"
+        " [options] --hostname --portno --numquery"
         "e.g.: \n"
         " nnet-forward nnet ark:features.ark ark:mlpoutput.ark\n";
 
@@ -43,6 +49,12 @@
     PdfPriorOptions prior_opts;
     prior_opts.Register(&po);
 
+    bool use_service = false;
+    po.Register("use-service", &use_service, "Will use the remote dnn service");
+
+    std::string write_model_file("");
+    po.Register("write-model", &write_model_file, "nnet-forward is wrtting weights and bias to file");
+
     std::string feature_transform;
     po.Register("feature-transform", &feature_transform, "Feature transform in front of main network (in nnet format)");
 
@@ -54,6 +66,16 @@
     std::string use_gpu="no";
     po.Register("use-gpu", &use_gpu, "yes|no|optional, only has effect if compiled with CUDA"); 
 
+
+    std::string hostname("");
+    po.Register("hostname", &hostname, "Server address of the NN service");
+     
+    int portno = -1;
+    po.Register("portno", &portno, "Port number of the NN service");
+
+    int numquery = 1;
+    po.Register("num-query", &numquery, "Number of query to put into one batch");
+
     po.Read(argc, argv);
 
     if (po.NumArgs() != 3) {
@@ -64,11 +86,11 @@
     std::string model_filename = po.GetArg(1),
         feature_rspecifier = po.GetArg(2),
         feature_wspecifier = po.GetArg(3);
-        
+
     using namespace kaldi;
     using namespace kaldi::nnet1;
     typedef kaldi::int32 int32;
-
+    
     //Select the GPU
 #if HAVE_CUDA==1
     CuDevice::Instantiate().SelectGpuId(use_gpu);
@@ -77,14 +99,14 @@
 
     Nnet nnet_transf;
     if (feature_transform != "") {
+      KALDI_LOG << "FEATURE TRANSFORM READIN STRING IS " << feature_transform; 
       nnet_transf.Read(feature_transform);
     }
 
     Nnet nnet;
     nnet.Read(model_filename);
     //optionally remove softmax
-    if (no_softmax && nnet.GetComponent(nnet.NumComponents()-1).GetType() ==
-        kaldi::nnet1::Component::kSoftmax) {
+    if (no_softmax && nnet.GetComponent(nnet.NumComponents()-1).GetType() == Component::kSoftmax) {
       KALDI_LOG << "Removing softmax from the nnet " << model_filename;
       nnet.RemoveComponent(nnet.NumComponents()-1);
     }
@@ -92,8 +114,7 @@
     if (apply_log && no_softmax) {
       KALDI_ERR << "Nonsense option combination : --apply-log=true and --no-softmax=true";
     }
-    if (apply_log && nnet.GetComponent(nnet.NumComponents()-1).GetType() !=
-        kaldi::nnet1::Component::kSoftmax) {
+    if (apply_log && nnet.GetComponent(nnet.NumComponents()-1).GetType() != Component::kSoftmax) {
       KALDI_ERR << "Used --apply-log=true, but nnet " << model_filename 
                 << " does not have <softmax> as last component!";
     }
@@ -108,6 +129,15 @@
     nnet_transf.SetDropoutRetention(1.0);
     nnet.SetDropoutRetention(1.0);
 
+  
+    if(write_model_file != ""){
+      // So we write the model to file
+      // and exit the program
+      Output ko(write_model_file, false); 
+      nnet.Write(ko.Stream(), false);
+      exit(0);
+    } 
+
     kaldi::int64 tot_t = 0;
 
     SequentialBaseFloatMatrixReader feature_reader(feature_rspecifier);
@@ -115,16 +145,19 @@
 
     CuMatrix<BaseFloat> feats, feats_transf, nnet_out;
     Matrix<BaseFloat> nnet_out_host;
-
+    Matrix<BaseFloat> recv_mat;
 
     Timer time;
     double time_now = 0;
+    double time_comm = 0;
+    double time_nn = 0;
     int32 num_done = 0;
+    
     // iterate over all feature files
     for (; !feature_reader.Done(); feature_reader.Next()) {
       // read
       const Matrix<BaseFloat> &mat = feature_reader.Value();
-      KALDI_VLOG(2) << "Processing utterance " << num_done+1 
+      KALDI_LOG << "Processing utterance " << num_done+1 
                     << ", " << feature_reader.Key() 
                     << ", " << mat.NumRows() << "frm";
 
@@ -136,10 +169,106 @@
       
       // push it to gpu
       feats = mat;
-      // fwd-pass
+
+      int app_input_size = 0;
+      for(MatrixIndexT i = 0; i < feats.NumRows(); i++){
+        app_input_size += sizeof(feats.Row(i).Data());
+      }
+      KALDI_LOG << "App input with dimensions: "<<feats.NumRows() << " and " << feats.NumCols();
+
+      KALDI_LOG << "App input data size is " << (float)app_input_size/(float)1024 << "KB";
       nnet_transf.Feedforward(feats, &feats_transf);
-      nnet.Feedforward(feats_transf, &nnet_out);
-      
+      KALDI_LOG << "Input feature with dimension: "<<feats_transf.NumCols(); 
+      if(use_service){
+        KALDI_LOG << "Use remote dnn service to inference.";
+        // Connect to the server
+        int socket;
+        char* hostname_cstr = new char [hostname.length() + 1];
+        std::strcpy(hostname_cstr, hostname.c_str());
+        socket = CLIENT_init(hostname_cstr, portno, DEBUG);
+        if(socket < 0){
+          KALDI_ERR << "Socket return as zero.";
+          exit(1);
+        }  
+        KALDI_LOG << "Establish socket with server at "
+                << hostname << ":" << portno;
+        // 1. Send the request type (1)
+        int req_type = 3;
+        SOCKET_send(socket, (char*)&req_type, sizeof(int), DEBUG);
+        KALDI_LOG << "Send request type: "<<req_type; 
+
+        // 2. Send the length of the input feature 
+        recv_mat.Resize(feats_transf.NumRows(), 1706);
+
+        int total_input_features = numquery * feats_transf.NumCols() * feats_transf.NumRows();
+        SOCKET_txsize(socket, total_input_features);
+
+        // The following block of code is commented out 
+        // and reserved for future reference
+        // This is the old implementation where features are sent vector by vector
+        /*
+        // 6. Start to send the feature and get result frame by frame
+        for(MatrixIndexT i = 0; i < feats_transf.NumRows(); i++){
+
+          Timer time_comm_temp;
+
+          int sent = SOCKET_send(socket, (char*) feats_transf.Row(i).Data(), 
+                          feats_transf.NumCols()*sizeof(float), DEBUG);
+          if(DEBUG){
+            KALDI_LOG<<"Sent "<<sent << " data";
+          }
+          // Receive neural network output
+          Vector<BaseFloat> cur_row(1706);
+          int rcvd = SOCKET_receive(socket, (char*) cur_row.Data(), 1706*sizeof(float), DEBUG);
+          if(DEBUG){
+            KALDI_LOG<<"Recv "<<rcvd << " data";
+          }
+          
+          time_comm += time_comm_temp.Elapsed();
+
+          recv_mat.CopyRowFromVec(cur_row, i);
+        }
+        */
+
+        // 3. Now we send the entire sentence over for batch processing
+        nnet_out.Resize(feats_transf.NumRows(), 1706); 
+        
+        Timer time_comm_temp;
+
+        for(int n = 0; n < numquery; n++){  
+          int total_sent = 0;
+          for(MatrixIndexT i = 0; i < feats_transf.NumRows(); i++){
+            int sent = SOCKET_send(socket, (char*)feats_transf.Row(i).Data(),
+                          feats_transf.NumCols() * sizeof(float), DEBUG);
+            total_sent += sent;
+          }
+          assert(total_sent == total_input_features*sizeof(float)/numquery && "Not sending enough features.");
+        }
+
+        // 4. Now we receive the result, again with the matrix as a whole
+        for(int n = 0; n < numquery; n++){
+          int total_rcvd = 0;
+          for(MatrixIndexT i = 0; i < feats_transf.NumRows(); i++){
+           int rcvd = SOCKET_receive(socket, (char*)nnet_out.Row(i).Data(),
+                          1706 * sizeof(float), DEBUG);
+            total_rcvd += rcvd; 
+          }
+          assert(total_rcvd == feats_transf.NumRows() * 1706 * sizeof(float) && "Not recving enough features");
+        }
+
+        time_comm += time_comm_temp.Elapsed();
+
+        // 5. Close the socket, don't need it anymore
+        SOCKET_close(socket,DEBUG);
+        KALDI_LOG << "DNN service finishes. Socket closed.";
+      }else{
+        // Use local(kaldi's) dnn to inference
+        KALDI_LOG << "Use local dnn service to inference.";
+        Timer nn_timer;
+        nnet.Feedforward(feats_transf, &nnet_out);
+        time_nn += nn_timer.Elapsed();
+      }
+
       // convert posteriors to log-posteriors
       if (apply_log) {
         nnet_out.ApplyLog();
@@ -163,10 +292,10 @@
             KALDI_ERR << "inf in NNet coutput of : " << feature_reader.Key();
         }
       }
-
-      // write
+      
+     // write
       feature_writer.Write(feature_reader.Key(), nnet_out_host);
-
+      
       // progress log
       if (num_done % 100 == 0) {
         time_now = time.Elapsed();
@@ -182,6 +311,9 @@
     KALDI_LOG << "Done " << num_done << " files" 
               << " in " << time.Elapsed()/60 << "min," 
               << " (fps " << tot_t/time.Elapsed() << ")"; 
+    KALDI_LOG << "Total app time is " << time_now*1000 << "ms";
+    KALDI_LOG << "Communication time is " << time_comm*1000 << "ms";
+    KALDI_LOG << "Local neural net time is " << time_nn*1000 << "ms";
 
 #if HAVE_CUDA==1
     if (kaldi::g_kaldi_verbose_level >= 1) {
