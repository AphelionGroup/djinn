--- nnet-forward.cc.old	2014-10-24 19:15:55.573778262 -0400
+++ nnet-forward.cc	2014-10-23 23:18:29.841957444 -0400
@@ -22,19 +22,26 @@
 #include "nnet/nnet-nnet.h"
 #include "nnet/nnet-loss.h"
 #include "nnet/nnet-pdf-prior.h"
+#include "nnet/nnet-rbm.h"
 #include "base/kaldi-common.h"
 #include "util/common-utils.h"
 #include "base/timer.h"
+#include "socket.h"
 
 
 int main(int argc, char *argv[]) {
   using namespace kaldi;
   using namespace kaldi::nnet1;
   try {
+
+    // YK
+    // Socket stuff to talk to the server
+   // 
     const char *usage =
         "Perform forward pass through Neural Network.\n"
         "\n"
         "Usage:  nnet-forward [options] <model-in> <feature-rspecifier> <feature-wspecifier>\n"
+        " [options] --hostname --portno"
         "e.g.: \n"
         " nnet-forward nnet ark:features.ark ark:mlpoutput.ark\n";
 
@@ -43,6 +50,12 @@
     PdfPriorOptions prior_opts;
     prior_opts.Register(&po);
 
+    bool use_service = false;
+    po.Register("use-service", &use_service, "Will use the remote dnn service");
+
+    std::string write_model_file("");
+    po.Register("write-model", &write_model_file, "nnet-forward is wrtting weights and bias to file");
+
     std::string feature_transform;
     po.Register("feature-transform", &feature_transform, "Feature transform in front of main network (in nnet format)");
 
@@ -54,6 +67,13 @@
     std::string use_gpu="no";
     po.Register("use-gpu", &use_gpu, "yes|no|optional, only has effect if compiled with CUDA"); 
 
+
+    std::string hostname("");
+    po.Register("hostname", &hostname, "Server address of the NN service");
+     
+    int portno = -1;
+    po.Register("portno", &portno, "Port number of the NN service");
+
     po.Read(argc, argv);
 
     if (po.NumArgs() != 3) {
@@ -64,11 +84,11 @@
     std::string model_filename = po.GetArg(1),
         feature_rspecifier = po.GetArg(2),
         feature_wspecifier = po.GetArg(3);
-        
+
     using namespace kaldi;
     using namespace kaldi::nnet1;
     typedef kaldi::int32 int32;
-
+    
     //Select the GPU
 #if HAVE_CUDA==1
     CuDevice::Instantiate().SelectGpuId(use_gpu);
@@ -83,8 +103,7 @@
     Nnet nnet;
     nnet.Read(model_filename);
     //optionally remove softmax
-    if (no_softmax && nnet.GetComponent(nnet.NumComponents()-1).GetType() ==
-        kaldi::nnet1::Component::kSoftmax) {
+    if (no_softmax && nnet.GetComponent(nnet.NumComponents()-1).GetType() == Component::kSoftmax) {
       KALDI_LOG << "Removing softmax from the nnet " << model_filename;
       nnet.RemoveComponent(nnet.NumComponents()-1);
     }
@@ -92,8 +111,7 @@
     if (apply_log && no_softmax) {
       KALDI_ERR << "Nonsense option combination : --apply-log=true and --no-softmax=true";
     }
-    if (apply_log && nnet.GetComponent(nnet.NumComponents()-1).GetType() !=
-        kaldi::nnet1::Component::kSoftmax) {
+    if (apply_log && nnet.GetComponent(nnet.NumComponents()-1).GetType() != Component::kSoftmax) {
       KALDI_ERR << "Used --apply-log=true, but nnet " << model_filename 
                 << " does not have <softmax> as last component!";
     }
@@ -108,6 +126,15 @@
     nnet_transf.SetDropoutRetention(1.0);
     nnet.SetDropoutRetention(1.0);
 
+  
+    if(write_model_file != ""){
+      // So we write the model to file
+      // and exit the program
+      Output ko(write_model_file, false); 
+      nnet.Write(ko.Stream(), false);
+      exit(0);
+    } 
+
     kaldi::int64 tot_t = 0;
 
     SequentialBaseFloatMatrixReader feature_reader(feature_rspecifier);
@@ -115,16 +142,17 @@
 
     CuMatrix<BaseFloat> feats, feats_transf, nnet_out;
     Matrix<BaseFloat> nnet_out_host;
-
+    Matrix<BaseFloat> recv_mat;
 
     Timer time;
     double time_now = 0;
     int32 num_done = 0;
+ 
     // iterate over all feature files
     for (; !feature_reader.Done(); feature_reader.Next()) {
       // read
       const Matrix<BaseFloat> &mat = feature_reader.Value();
-      KALDI_VLOG(2) << "Processing utterance " << num_done+1 
+      KALDI_LOG << "Processing utterance " << num_done+1 
                     << ", " << feature_reader.Key() 
                     << ", " << mat.NumRows() << "frm";
 
@@ -137,8 +165,45 @@
       // push it to gpu
       feats = mat;
       // fwd-pass
+      // Preprocessing feature transformation      
       nnet_transf.Feedforward(feats, &feats_transf);
-      nnet.Feedforward(feats_transf, &nnet_out);
+      
+      if(use_service){
+        KALDI_LOG << "Use remote dnn service to inference.";
+        // Connect to the server
+        int socket;
+        char* hostname_cstr = new char [hostname.length() + 1];
+        std::strcpy(hostname_cstr, hostname.c_str());
+        socket = CLIENT_init(hostname_cstr, portno);
+        if(socket < 0){
+          KALDI_ERR << "Socket return as zero.";
+          exit(1);
+        }  
+        KALDI_LOG << "Establish socket with server at "
+                << hostname << ":" << portno;
+
+        recv_mat.Resize(feats_transf.NumRows(), 1706);
+        // Send features to the service frame by frame
+        SOCKET_txsize(socket, feats_transf.NumCols());
+        for(MatrixIndexT i = 0; i < feats_transf.NumRows(); i++){
+          SOCKET_send(socket, feats_transf.Row(i).Data(), feats_transf.NumCols());
+
+          // Receive neural network output
+          Vector<BaseFloat> cur_row(1706);
+          SOCKET_receive(socket, cur_row.Data(), 1706);
+          recv_mat.CopyRowFromVec(cur_row, i);
+        }
+        nnet_out.Resize(recv_mat.NumRows(), recv_mat.NumCols()); 
+        nnet_out.CopyFromMat(recv_mat);
+     
+        // Close the socket, don't need it anymore
+        SOCKET_close(socket);
+        KALDI_LOG << "DNN service finishes. Socket closed.";
+      }else{
+        // Use local(kaldi's) dnn to inference
+        KALDI_LOG << "Use local dnn service to inference.";
+        nnet.Feedforward(feats_transf, &nnet_out);
+      }
       
       // convert posteriors to log-posteriors
       if (apply_log) {
